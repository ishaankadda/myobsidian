**Leaky Abstraction**
A leaky abstraction is a design flaw in a system that causes some of its hidden underlying complexity to become exposed, forcing the user to have knowledge of said complexity for effective use and troubleshooting.
	*all non-trivial abstractions, to some degree, are leaky* 
	~Joel Spolsky, The Law of Leaky Abstractions, 2010
In 2020, [Massachusetts Institute of Technology](https://en.wikipedia.org/wiki/Massachusetts_Institute_of_Technology "Massachusetts Institute of Technology") computing science teaching staff Anish, Jose, and Jon argued that the command line interface for [git](https://en.wikipedia.org/wiki/Git "Git") is a leaky abstraction, in which the underlying "beautiful design" of the git data model needs to be understood for effective usage of git.

---

**Yes you should understand backprop -** [karpathy.medium.com](https://karpathy.medium.com/?source=post_page---byline--e2f06eab496b--------------------------------)
> “Why do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?”

**\> The problem with Backpropagation is that it is a** [**leaky abstraction**](https://en.wikipedia.org/wiki/Leaky_abstraction)**.**

In other words, it is easy to fall into the trap of abstracting away the learning process — believing that you can simply stack arbitrary layers together and backprop will “magically make them work” on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.
- Vanishing gradients on sigmoids
- Dead ReLU problem
- Exploding Gradients in Vanilla RNN - see picture below
- DQN clipping - 2016 github issue
![](https://miro.medium.com/v2/resize:fit:700/1*dqlX0ixpk1O3225bZ1LGnA.png)

---
